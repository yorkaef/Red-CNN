{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99069b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder1 = './skin-cancer-mnist-ham10000/HAM10000_images_part_1'\n",
    "folder2 = './skin-cancer-mnist-ham10000/HAM10000_images_part_2'\n",
    "\n",
    "images1 = [os.path.join(folder1, f) for f in os.listdir(folder1) if f.endswith('.jpg')]\n",
    "images2 = [os.path.join(folder2, f) for f in os.listdir(folder2) if f.endswith('.jpg')]\n",
    "\n",
    "random_images1 = random.sample(images1, 5)\n",
    "random_images2 = random.sample(images2, 5)\n",
    "\n",
    "def display_images(images, title):\n",
    "    plt.figure(figsize=(15, 3))  #Ajustar el tama침o de la figura.\n",
    "    for i, img_path in enumerate(images):\n",
    "        img = Image.open(img_path)\n",
    "        plt.subplot(1, 5, i + 1)  # Crear un subplot para cada imagen\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # No mostrar los ejes\n",
    "        plt.title(f\"Imagen {i + 1}\")\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "display_images(random_images1, \"5 imagenes aletorias de la carpeta 1\")\n",
    "display_images(random_images2, \"5 imagenes aletorias de la carpeta 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a374733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data = pd.read_csv('./skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n",
    "\n",
    "print(tabular_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./skin-cancer-mnist-ham10000/hmnist_28_28_RGB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('label', axis=1)\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Configurar semillas para reproducibilidad\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Funci칩n para balancear datos usando sklearn (alternativa a imblearn)\n",
    "def balance_data(x, y, random_state=SEED):\n",
    "    \"\"\"Balancea los datos usando oversampling\"\"\"\n",
    "    # Convertir a numpy si es pandas DataFrame\n",
    "    if hasattr(x, 'values'):\n",
    "        x_array = x.values\n",
    "    else:\n",
    "        x_array = np.array(x)\n",
    "    \n",
    "    if hasattr(y, 'values'):\n",
    "        y_array = y.values\n",
    "    else:\n",
    "        y_array = np.array(y)\n",
    "    \n",
    "    unique_classes, counts = np.unique(y_array, return_counts=True)\n",
    "    max_count = max(counts)\n",
    "    \n",
    "    x_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for class_label in unique_classes:\n",
    "        class_indices = np.where(y_array == class_label)[0]\n",
    "        class_x = x_array[class_indices]\n",
    "        class_y = y_array[class_indices]\n",
    "        \n",
    "        # Oversample para igualar a la clase mayoritaria\n",
    "        if len(class_x) < max_count:\n",
    "            class_x_resampled, class_y_resampled = resample(\n",
    "                class_x, class_y, \n",
    "                n_samples=max_count, \n",
    "                random_state=random_state\n",
    "            )\n",
    "        else:\n",
    "            class_x_resampled, class_y_resampled = class_x, class_y\n",
    "            \n",
    "        x_balanced.append(class_x_resampled)\n",
    "        y_balanced.append(class_y_resampled)\n",
    "    \n",
    "    return np.vstack(x_balanced), np.hstack(y_balanced)\n",
    "\n",
    "# Aplicar balanceado de datos\n",
    "x_resampled, y_resampled = balance_data(x, y)\n",
    "x_resampled = np.array(x_resampled).reshape(-1, 28, 28, 3).astype(np.float32)\n",
    "\n",
    "# Normalizaci칩n\n",
    "mean = np.mean(x_resampled)\n",
    "std = np.std(x_resampled)\n",
    "x_resampled = (x_resampled - mean) / std\n",
    "\n",
    "# Seleccionar muestras para augmentation\n",
    "num_augmented_samples = int(0.005 * len(x_resampled))\n",
    "rng = np.random.default_rng(SEED)\n",
    "indices = rng.choice(len(x_resampled), num_augmented_samples, replace=False)\n",
    "x_selected = x_resampled[indices]\n",
    "y_selected = y_resampled[indices]\n",
    "\n",
    "# Data Augmentation usando TensorFlow\n",
    "def augment_images(images):\n",
    "    \"\"\"Aplica data augmentation usando TensorFlow\"\"\"\n",
    "    augmented_images = []\n",
    "    \n",
    "    for img in images:\n",
    "        img = tf.expand_dims(img, 0)\n",
    "        \n",
    "        if tf.random.uniform([]) > 0.5:\n",
    "            img = tf.image.flip_left_right(img)\n",
    "        \n",
    "        if tf.random.uniform([]) > 0.5:\n",
    "            img = tf.image.flip_up_down(img)\n",
    "        \n",
    "        if tf.random.uniform([]) > 0.5:\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], minval=0, maxval=4, dtype=tf.int32))\n",
    "        \n",
    "        img = tf.image.random_brightness(img, max_delta=0.1)\n",
    "        img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
    "        img = tf.image.random_saturation(img, lower=0.9, upper=1.1)\n",
    "        \n",
    "        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.01)\n",
    "        img = img + noise\n",
    "        \n",
    "        img = tf.image.random_crop(img, size=[1, 25, 25, 3])\n",
    "        img = tf.image.resize(img, [28, 28])\n",
    "        img = tf.clip_by_value(img, -3.0, 3.0)\n",
    "        \n",
    "        augmented_images.append(tf.squeeze(img, 0))\n",
    "    \n",
    "    return tf.stack(augmented_images)\n",
    "\n",
    "# Aplicar data augmentation\n",
    "x_augmented = augment_images(x_selected)\n",
    "\n",
    "# Combinar datos originales y aumentados\n",
    "x_resampled = np.concatenate((x_resampled, x_augmented.numpy()), axis=0)\n",
    "y_resampled = np.concatenate((y_resampled, y_selected), axis=0)\n",
    "\n",
    "# Divisi칩n en entrenamiento y prueba\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    x_resampled, y_resampled, test_size=0.2, random_state=SEED, stratify=y_resampled\n",
    ")\n",
    "\n",
    "print(f'Training set size: {X_train.shape}, Testing set size: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe52c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "class CustomLearningRateScheduler(Callback):\n",
    "    def __init__(self, h=1.0, m_range=10, patience=3, random_factor=0.1, decay_rate=0.98, warmup_epochs=5, min_lr=1e-6, max_adjustment=0.0005, verbose=0):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.h = h\n",
    "        self.m_range = m_range\n",
    "        self.patience = patience\n",
    "        self.random_factor = random_factor\n",
    "        self.decay_rate = decay_rate\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.min_lr = min_lr\n",
    "        self.max_adjustment = max_adjustment\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = float('inf')\n",
    "        self.loss_history = []\n",
    "\n",
    "    def compute_adjustment_factor(self, logs):\n",
    "        current_loss = logs.get(\"loss\", 1.0)\n",
    "        g = lambda x: tf.nn.relu(tf.cast(x, tf.float32))\n",
    "\n",
    "        adjustment_factor = 0.0\n",
    "        for m in range(-self.m_range, self.m_range + 1):\n",
    "            shift = m * self.h\n",
    "            left_shift = g(-shift)\n",
    "            right_shift = g(shift)\n",
    "            adjustment_factor += current_loss * (left_shift - right_shift)\n",
    "\n",
    "        adjustment_factor *= (self.h / 2)\n",
    "        adjustment_factor = np.clip(adjustment_factor.numpy(), -self.max_adjustment, self.max_adjustment)\n",
    "        return adjustment_factor\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "\n",
    "        optimizer = self.model.optimizer\n",
    "        try:\n",
    "            if isinstance(optimizer.learning_rate, tf.Variable):\n",
    "                current_lr = tf.keras.backend.get_value(optimizer.learning_rate)\n",
    "            elif hasattr(optimizer.learning_rate, \"numpy\"):\n",
    "                current_lr = optimizer.learning_rate.numpy()\n",
    "            else:\n",
    "                current_lr = optimizer.learning_rate\n",
    "        except Exception as e:\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Epoch {epoch+1}: Unable to retrieve learning rate. Error: {e}\")\n",
    "            return\n",
    "\n",
    "        adjustment_factor = self.compute_adjustment_factor(logs)\n",
    "        self.loss_history.append(logs.get(\"loss\", 1.0))\n",
    "        if len(self.loss_history) > 3:\n",
    "            recent_losses = np.array(self.loss_history[-3:])\n",
    "            if np.all(recent_losses[1:] > recent_losses[:-1]):\n",
    "                adjustment_factor *= 0.5\n",
    "\n",
    "        if epoch < self.warmup_epochs:\n",
    "            new_lr = current_lr * (1 + 0.2 * (epoch / self.warmup_epochs))\n",
    "        else:\n",
    "            new_lr = current_lr * self.decay_rate + adjustment_factor\n",
    "\n",
    "        random_adjustment = 1 + rng.uniform(-self.random_factor, self.random_factor)\n",
    "        new_lr *= random_adjustment\n",
    "        new_lr = max(self.min_lr, new_lr)\n",
    "\n",
    "        try:\n",
    "            if isinstance(optimizer.learning_rate, tf.Variable):\n",
    "                tf.keras.backend.set_value(optimizer.learning_rate, new_lr)\n",
    "            else:\n",
    "                optimizer.learning_rate = new_lr\n",
    "        except Exception as e:\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Epoch {epoch+1}: Unable to set learning rate. Error: {e}\")\n",
    "            return\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(f\"Epoch {epoch+1}: Adjusted LR to {new_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "\n",
    "# Define the LeNet-5 model\n",
    "model = Sequential([\n",
    "    # First convolutional layer (6 filters, 5x5 kernel, tanh activation)\n",
    "    Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', input_shape=(28, 28, 3), padding='same'),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Second convolutional layer (16 filters, 5x5 kernel, tanh activation)\n",
    "    Conv2D(filters=16, kernel_size=(5, 5), activation='tanh'),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten the feature maps\n",
    "    Flatten(),\n",
    "\n",
    "    # First fully connected layer (120 units, tanh activation)\n",
    "    Dense(120, activation='tanh'),\n",
    "\n",
    "    # Second fully connected layer (84 units, tanh activation)\n",
    "    Dense(84, activation='tanh'),\n",
    "\n",
    "    # Output layer (7 units, softmax activation for 7 classes)\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7469a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Initial learning rate\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "custom_lr_scheduler = CustomLearningRateScheduler(h=1.0, m_range=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_train_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    callbacks=[custom_lr_scheduler]\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_train_time\n",
    "print(f\" training time: {train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aaaaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model.evaluate(X_train, Y_train, verbose= 1)\n",
    "test_score = model.evaluate(X_test, Y_test, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "if 'accuracy' in history.history:  # For models using accuracy metric\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the baseline model\n",
    "model1 = Sequential([\n",
    "    # First convolutional layer (6 filters, 5x5 kernel, tanh activation)\n",
    "    Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', input_shape=(28, 28, 3), padding='same'),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Second convolutional layer (16 filters, 5x5 kernel, tanh activation)\n",
    "    Conv2D(filters=16, kernel_size=(5, 5), activation='tanh'),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten the feature maps\n",
    "    Flatten(),\n",
    "\n",
    "    # First fully connected layer (120 units, tanh activation)\n",
    "    Dense(120, activation='tanh'),\n",
    "\n",
    "    # Second fully connected layer (84 units, tanh activation)\n",
    "    Dense(84, activation='tanh'),\n",
    "\n",
    "    # Output layer (7 units, softmax activation for 7 classes)\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "\n",
    "# Instantiate the custom learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Triangular Learning Rate Policy\n",
    "def triangular_lr(epoch, max_lr=0.00178, min_lr=0.00077, stepsize=10):\n",
    "    \"\"\"Implements the Triangular CLR policy.\"\"\"\n",
    "    cycle = np.floor(1 + epoch / (2 * stepsize))\n",
    "    x = np.abs(epoch / stepsize - 2 * cycle + 1)\n",
    "    lr = min_lr + (max_lr - min_lr) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "# Define the LearningRateScheduler Callback\n",
    "lr_callback = LearningRateScheduler(lambda epoch: triangular_lr(epoch, max_lr=0.00146, min_lr=0.00088, stepsize=10))\n",
    "\n",
    "\n",
    "model1.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c0cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint(\n",
    "    filepath='best_model.keras',  # Changed to .keras extension\n",
    "    monitor='val_accuracy',       # Monitoring validation accuracy\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2448f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train_time = time.time()\n",
    "\n",
    "history = model1.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    callbacks=[lr_callback]\n",
    ")\n",
    "train_time = time.time() - start_train_time\n",
    "print(f\" training time: {train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model1.evaluate(X_train, Y_train, verbose= 1)\n",
    "test_score = model1.evaluate(X_test, Y_test, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "if 'accuracy' in history.history:  # For models using accuracy metric\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6711b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "\n",
    "# Define the LeNet-4 model\n",
    "model2 = Sequential([\n",
    "    # First convolutional layer (6 filters, 5x5 kernel, tanh activation)\n",
    "    Conv2D(filters=4, kernel_size=(5, 5), activation='tanh', input_shape=(28, 28, 3), padding='same'),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Second convolutional layer (16 filters, 5x5 kernel, tanh activation)\n",
    "    Conv2D(filters=8, kernel_size=(5, 5), activation='tanh'),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten the feature maps\n",
    "    Flatten(),\n",
    "\n",
    "\n",
    "\n",
    "    # Output layer (7 units, softmax activation for 7 classes)\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Summary of the model\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09909553",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Initial learning rate\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73ac84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lr_scheduler = CustomLearningRateScheduler(h=1.0, m_range=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8edd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_train_time = time.time()\n",
    "\n",
    "history = model2.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    callbacks=[custom_lr_scheduler]\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_train_time\n",
    "print(f\" training time: {train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83051474",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model2.evaluate(X_train, Y_train, verbose= 1)\n",
    "test_score = model2.evaluate(X_test, Y_test, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abe0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "if 'accuracy' in history.history:  # For models using accuracy metric\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437eb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "\n",
    "# Define the LeNet-4 model\n",
    "model3 = Sequential([\n",
    "    # First convolutional layer (6 filters, 5x5 kernel, tanh activation)\n",
    "    Conv2D(filters=4, kernel_size=(5, 5), activation='tanh', input_shape=(28, 28, 3), padding='same'),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Second convolutional layer (16 filters, 5x5 kernel, tanh activation)\n",
    "    Conv2D(filters=8, kernel_size=(5, 5), activation='tanh'),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten the feature maps\n",
    "    Flatten(),\n",
    "\n",
    "\n",
    "\n",
    "    # Output layer (7 units, softmax activation for 7 classes)\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Summary of the model\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint(\n",
    "    filepath='best_model.keras',  # Changed to .keras extension\n",
    "    monitor='val_accuracy',       # Monitoring validation accuracy\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458aa2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model3.evaluate(X_train, Y_train, verbose= 1)\n",
    "test_score = model3.evaluate(X_test, Y_test, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "if 'accuracy' in history.history:  # For models using accuracy metric\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "\n",
    "# Make predictions on training set (or validation set)\n",
    "y_pred = model1.predict(X_train)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted class labels\n",
    "\n",
    "# Check the shape of Y_train\n",
    "if Y_train.ndim == 2:  # If Y_train is one-hot encoded\n",
    "    y_true = np.argmax(Y_train, axis=1)  # Convert one-hot encoded labels to class indices\n",
    "else:  # If Y_train is already in class index format\n",
    "    y_true = Y_train  # Use it directly\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(7), yticklabels=np.arange(7))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "if 'accuracy' in history.history:  # For models using accuracy metric\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "\n",
    "# Assuming model1 is already trained and X_train, Y_train are defined\n",
    "# Make predictions on training set (or validation set)\n",
    "y_pred = model1.predict(X_train)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted class labels\n",
    "\n",
    "# Check the shape of Y_train\n",
    "if Y_train.ndim == 2:  # If Y_train is one-hot encoded\n",
    "    y_true = np.argmax(Y_train, axis=1)  # Convert one-hot encoded labels to class indices\n",
    "else:  # If Y_train is already in class index format\n",
    "    y_true = Y_train  # Use it directly\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(7), yticklabels=np.arange(7))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate precision, recall, and accuracy\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Optionally, print a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=[f'Class {i}' for i in range(7)]))\n",
    "\n",
    "# Plotting the loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "if 'accuracy' in history.history:  # For models using accuracy metric\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc664a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
