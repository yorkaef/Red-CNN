{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, AveragePooling2D, \n",
    "                                     Flatten, Dense, Dropout, BatchNormalization,\n",
    "                                     GlobalAveragePooling2D)\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, EarlyStopping, \n",
    "                                        ReduceLROnPlateau, LearningRateScheduler)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(f\"Versión de TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8383d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_display_sample_images():\n",
    "    \"\"\"Cargar y mostrar imágenes de muestra de ambas carpetas\"\"\"\n",
    "    folder1 = './skin-cancer-mnist-ham10000/HAM10000_images_part_1'\n",
    "    folder2 = './skin-cancer-mnist-ham10000/HAM10000_images_part_2'\n",
    "    \n",
    "    if os.path.exists(folder1) and os.path.exists(folder2):\n",
    "        images1 = [os.path.join(folder1, f) for f in os.listdir(folder1) if f.endswith('.jpg')]\n",
    "        images2 = [os.path.join(folder2, f) for f in os.listdir(folder2) if f.endswith('.jpg')]\n",
    "        \n",
    "        random_images1 = random.sample(images1, min(5, len(images1)))\n",
    "        random_images2 = random.sample(images2, min(5, len(images2)))\n",
    "        \n",
    "        def display_images(images, title):\n",
    "            plt.figure(figsize=(15, 3))\n",
    "            for i, img_path in enumerate(images):\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    plt.subplot(1, 5, i + 1)\n",
    "                    plt.imshow(img)\n",
    "                    plt.axis('off')\n",
    "                    plt.title(f\"Imagen {i + 1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cargando imagen {img_path}: {e}\")\n",
    "            plt.suptitle(title)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        display_images(random_images1, \"5 imágenes aleatorias de la carpeta 1\")\n",
    "        display_images(random_images2, \"5 imágenes aleatorias de la carpeta 2\")\n",
    "    else:\n",
    "        print(\"Carpetas de imágenes no encontradas. Saltando visualización de imágenes.\")\n",
    "\n",
    "# Ejecutar la función\n",
    "load_and_display_sample_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a8989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata():\n",
    "    \"\"\"Cargar y mostrar metadatos si están disponibles\"\"\"\n",
    "    try:\n",
    "        tabular_data = pd.read_csv('./skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n",
    "        print(\"Metadatos cargados exitosamente:\")\n",
    "        print(tabular_data.head())\n",
    "        print(f\"\\nInformación del conjunto de datos:\")\n",
    "        print(f\"Forma: {tabular_data.shape}\")\n",
    "        print(f\"Columnas: {tabular_data.columns.tolist()}\")\n",
    "        return tabular_data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Archivo de metadatos no encontrado. Continuando sin metadatos.\")\n",
    "        return None\n",
    "\n",
    "# Cargar metadatos\n",
    "metadata = load_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    \"\"\"Cargar los datos preprocesados del CSV\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv('./skin-cancer-mnist-ham10000/hmnist_28_28_RGB.csv')\n",
    "        print(f\"Datos cargados exitosamente. Forma: {data.shape}\")\n",
    "        print(f\"Columnas: {data.columns.tolist()}\")\n",
    "        print(f\"Distribución de etiquetas:\")\n",
    "        print(data['label'].value_counts().sort_index())\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Archivo CSV no encontrado. Por favor verifica la ruta del archivo.\")\n",
    "        return None\n",
    "\n",
    "# Cargar datos principales\n",
    "data = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar características y etiquetas\n",
    "X = data.drop('label', axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "print(f\"Forma de datos originales: {X.shape}\")\n",
    "print(f\"Forma de etiquetas: {y.shape}\")\n",
    "print(f\"Clases: {np.unique(y)}\")\n",
    "print(f\"Número de clases: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_balance_data(x, y, method='oversample', random_state=SEED):\n",
    "    \"\"\"\n",
    "    Advanced data balancing with multiple strategies\n",
    "    \"\"\"\n",
    "    if hasattr(x, 'values'):\n",
    "        x_array = x.values\n",
    "    else:\n",
    "        x_array = np.array(x)\n",
    "    \n",
    "    if hasattr(y, 'values'):\n",
    "        y_array = y.values\n",
    "    else:\n",
    "        y_array = np.array(y)\n",
    "    \n",
    "    unique_classes, counts = np.unique(y_array, return_counts=True)\n",
    "    print(f\"Distribución de clases original: {dict(zip(unique_classes, counts))}\")\n",
    "    \n",
    "    if method == 'oversample':\n",
    "        # Sobremuestreo de clases minoritarias\n",
    "        max_count = max(counts)\n",
    "        x_balanced = []\n",
    "        y_balanced = []\n",
    "        \n",
    "        for class_label in unique_classes:\n",
    "            class_indices = np.where(y_array == class_label)[0]\n",
    "            class_x = x_array[class_indices]\n",
    "            class_y = y_array[class_indices]\n",
    "            \n",
    "            if len(class_x) < max_count:\n",
    "                class_x_resampled, class_y_resampled = resample(\n",
    "                    class_x, class_y, \n",
    "                    n_samples=max_count, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "            else:\n",
    "                class_x_resampled, class_y_resampled = class_x, class_y\n",
    "                \n",
    "            x_balanced.append(class_x_resampled)\n",
    "            y_balanced.append(class_y_resampled)\n",
    "        \n",
    "        x_final = np.vstack(x_balanced)\n",
    "        y_final = np.hstack(y_balanced)\n",
    "        \n",
    "        print(f\"Distribución de clases balanceada: {dict(zip(*np.unique(y_final, return_counts=True)))}\")\n",
    "        return x_final, y_final\n",
    "\n",
    "# Aplicar balanceo de datos\n",
    "X_balanced, y_balanced = advanced_balance_data(X, y, method='oversample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb158c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape and normalize\n",
    "X_balanced = X_balanced.reshape(-1, 28, 28, 3).astype(np.float32)\n",
    "\n",
    "# Advanced normalization\n",
    "mean = np.mean(X_balanced, axis=(0, 1, 2), keepdims=True)\n",
    "std = np.std(X_balanced, axis=(0, 1, 2), keepdims=True)\n",
    "X_balanced = (X_balanced - mean) / (std + 1e-8)\n",
    "\n",
    "print(f\"Balanced data shape: {X_balanced.shape}\")\n",
    "print(f\"Data range after normalization: [{X_balanced.min():.3f}, {X_balanced.max():.3f}]\")\n",
    "print(f\"Mean: {X_balanced.mean():.3f}, Std: {X_balanced.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990693e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_augmentation_layer():\n",
    "    \"\"\"Create a data augmentation layer using tf.keras\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomBrightness(0.1),\n",
    "        tf.keras.layers.RandomContrast(0.1),\n",
    "    ])\n",
    "\n",
    "def advanced_augment_images(images, labels, augment_factor=0.3):\n",
    "    \"\"\"Advanced data augmentation with multiple techniques\"\"\"\n",
    "    num_augmented = int(augment_factor * len(images))\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    indices = rng.choice(len(images), num_augmented, replace=False)\n",
    "    \n",
    "    x_selected = images[indices]\n",
    "    y_selected = labels[indices]\n",
    "    \n",
    "    # Create augmentation pipeline\n",
    "    augmentation_layer = create_data_augmentation_layer()\n",
    "    \n",
    "    # Apply augmentation\n",
    "    x_augmented = []\n",
    "    for img in x_selected:\n",
    "        img_expanded = tf.expand_dims(img, 0)\n",
    "        aug_img = augmentation_layer(img_expanded, training=True)\n",
    "        \n",
    "        # Add noise\n",
    "        noise = tf.random.normal(shape=tf.shape(aug_img), mean=0.0, stddev=0.01)\n",
    "        aug_img = aug_img + noise\n",
    "        \n",
    "        # Clip values\n",
    "        aug_img = tf.clip_by_value(aug_img, -3.0, 3.0)\n",
    "        x_augmented.append(tf.squeeze(aug_img, 0))\n",
    "    \n",
    "    x_augmented = tf.stack(x_augmented)\n",
    "    return x_augmented.numpy(), y_selected\n",
    "\n",
    "# Apply data augmentation\n",
    "print(\"Applying data augmentation...\")\n",
    "X_aug, y_aug = advanced_augment_images(X_balanced, y_balanced, augment_factor=0.2)\n",
    "\n",
    "# Combine original and augmented data\n",
    "X_final = np.concatenate([X_balanced, X_aug], axis=0)\n",
    "y_final = np.concatenate([y_balanced, y_aug], axis=0)\n",
    "\n",
    "print(f\"Final dataset shape: {X_final.shape}\")\n",
    "print(f\"Original data: {X_balanced.shape[0]} samples\")\n",
    "print(f\"Augmented data: {X_aug.shape[0]} samples\")\n",
    "print(f\"Total data: {X_final.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5399d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_final, y_final, test_size=0.3, random_state=SEED, stratify=y_final\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution in each set\n",
    "print(f\"\\nTraining set class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "print(f\"Validation set class distribution: {dict(zip(*np.unique(y_val, return_counts=True)))}\")\n",
    "print(f\"Test set class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554127d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_lenet(input_shape=(28, 28, 3), num_classes=7, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Improved LeNet-5 with modern techniques\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First convolutional block\n",
    "        Conv2D(32, (5, 5), activation='relu', input_shape=input_shape, \n",
    "               kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate * 0.5),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        Conv2D(64, (5, 5), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate * 0.5),\n",
    "        \n",
    "        # Third convolutional block (additional)\n",
    "        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate * 0.5),\n",
    "        \n",
    "        # Global Average Pooling instead of Flatten\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the improved LeNet model\n",
    "model_improved = create_improved_lenet()\n",
    "model_improved.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lightweight_cnn(input_shape=(28, 28, 3), num_classes=7, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Lightweight CNN for comparison\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create lightweight CNN model\n",
    "model_lightweight = create_lightweight_cnn()\n",
    "model_lightweight.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8aa86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_cnn(input_shape=(28, 28, 3), num_classes=7, dropout_rate=0.4):\n",
    "    \"\"\"\n",
    "    Deeper CNN with residual-like connections\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Block 1\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate * 0.5),\n",
    "        \n",
    "        # Block 2\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate * 0.5),\n",
    "        \n",
    "        # Block 3\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate * 0.5),\n",
    "        \n",
    "        # Classification head\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create deep CNN model\n",
    "model_deep = create_deep_cnn()\n",
    "model_deep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef286b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_callbacks(model_name):\n",
    "    \"\"\"Create modern callbacks for training\"\"\"\n",
    "    callbacks = [\n",
    "        # Model checkpoint\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'best_{model_name}.keras',\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Early stopping\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Learning rate reduction\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "def cosine_decay_with_warmup(epoch, total_epochs=50, warmup_epochs=5, \n",
    "                           initial_lr=0.001, min_lr=1e-6):\n",
    "    \"\"\"\n",
    "    Cosine decay learning rate schedule with warmup\n",
    "    \"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warmup\n",
    "        lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        # Cosine decay\n",
    "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "        lr = min_lr + (initial_lr - min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    return lr\n",
    "\n",
    "# Test the learning rate schedule\n",
    "epochs_test = np.arange(30)\n",
    "lrs_test = [cosine_decay_with_warmup(epoch, total_epochs=30) for epoch in epochs_test]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(epochs_test, lrs_test, linewidth=2)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Cosine Decay Learning Rate Schedule with Warmup')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile improved LeNet model\n",
    "model_improved.compile(\n",
    "    optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Create callbacks\n",
    "callbacks_improved = create_advanced_callbacks(\"improved_lenet\")\n",
    "\n",
    "# Add learning rate scheduler\n",
    "lr_scheduler = LearningRateScheduler(\n",
    "    lambda epoch: cosine_decay_with_warmup(epoch, total_epochs=30)\n",
    ")\n",
    "callbacks_improved.append(lr_scheduler)\n",
    "\n",
    "print(\"Training Improved LeNet...\")\n",
    "print(f\"Model parameters: {model_improved.count_params():,}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the improved model\n",
    "history_improved = model_improved.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks_improved,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time_improved = time.time() - start_time\n",
    "print(f\"Training completed in {training_time_improved:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152adcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lightweight.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Create callbacks\n",
    "callbacks_lightweight = create_advanced_callbacks(\"lightweight_cnn\")\n",
    "callbacks_lightweight.append(\n",
    "    LearningRateScheduler(lambda epoch: cosine_decay_with_warmup(epoch, total_epochs=30))\n",
    ")\n",
    "\n",
    "print(\"Training Lightweight CNN...\")\n",
    "print(f\"Model parameters: {model_lightweight.count_params():,}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the lightweight model\n",
    "history_lightweight = model_lightweight.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks_lightweight,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time_lightweight = time.time() - start_time\n",
    "print(f\"Training completed in {training_time_lightweight:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile deep model\n",
    "model_deep.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Create callbacks\n",
    "callbacks_deep = create_advanced_callbacks(\"deep_cnn\")\n",
    "callbacks_deep.append(\n",
    "    LearningRateScheduler(lambda epoch: cosine_decay_with_warmup(epoch, total_epochs=30))\n",
    ")\n",
    "\n",
    "print(\"Training Deep CNN...\")\n",
    "print(f\"Model parameters: {model_deep.count_params():,}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the deep model\n",
    "history_deep = model_deep.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks_deep,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time_deep = time.time() - start_time\n",
    "print(f\"Training completed in {training_time_deep:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c04ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(model, X_test, Y_test, X_train, Y_train, \n",
    "                           history, model_name, class_names=None):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"EVALUATION RESULTS FOR {model_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Evaluate on train and test sets\n",
    "    train_loss, train_acc = model.evaluate(X_train, Y_train, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    \n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Generalization Gap: {train_acc - test_acc:.4f}\")\n",
    "    \n",
    "    # Predictions for confusion matrix\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Classification report\n",
    "    if class_names is None:\n",
    "        class_names = [f'Class {i}' for i in range(7)]\n",
    "    \n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(Y_test, y_pred_classes, target_names=class_names))\n",
    "    \n",
    "    return {\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'generalization_gap': train_acc - test_acc,\n",
    "        'predictions': y_pred_classes\n",
    "    }\n",
    "\n",
    "# Define class names\n",
    "class_names = ['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b89b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate improved LeNet\n",
    "results_improved = comprehensive_evaluation(\n",
    "    model_improved, X_test, y_test, X_train, y_train, \n",
    "    history_improved, \"Improved LeNet\", class_names\n",
    ")\n",
    "\n",
    "results_improved['training_time'] = training_time_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate lightweight CNN\n",
    "results_lightweight = comprehensive_evaluation(\n",
    "    model_lightweight, X_test, y_test, X_train, y_train, \n",
    "    history_lightweight, \"Lightweight CNN\", class_names\n",
    ")\n",
    "\n",
    "results_lightweight['training_time'] = training_time_lightweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ae1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate deep CNN\n",
    "results_deep = comprehensive_evaluation(\n",
    "    model_deep, X_test, y_test, X_train, y_train, \n",
    "    history_deep, \"Deep CNN\", class_names\n",
    ")\n",
    "\n",
    "results_deep['training_time'] = training_time_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for improved LeNet\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Improved LeNet - Training Results and Analysis', fontsize=16)\n",
    "\n",
    "# Training history\n",
    "axes[0, 0].plot(history_improved.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(history_improved.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epochs')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history_improved.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(history_improved.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epochs')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, results_improved['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[1, 0].set_xlabel('Predicted Label')\n",
    "axes[1, 0].set_ylabel('True Label')\n",
    "axes[1, 0].set_title('Confusion Matrix')\n",
    "\n",
    "# Learning rate schedule\n",
    "if 'lr' in history_improved.history:\n",
    "    axes[1, 1].plot(history_improved.history['lr'], linewidth=2, color='red')\n",
    "    axes[1, 1].set_xlabel('Epochs')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].set_title('Learning Rate Schedule')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    # Class distribution\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    axes[1, 1].bar(range(len(class_names)), counts, color='skyblue', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Classes')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    axes[1, 1].set_title('Test Set Class Distribution')\n",
    "    axes[1, 1].set_xticks(range(len(class_names)))\n",
    "    axes[1, 1].set_xticklabels(class_names, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for lightweight CNN\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Lightweight CNN - Training Results and Analysis', fontsize=16)\n",
    "\n",
    "# Training history\n",
    "axes[0, 0].plot(history_lightweight.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(history_lightweight.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epochs')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history_lightweight.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(history_lightweight.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epochs')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, results_lightweight['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[1, 0],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[1, 0].set_xlabel('Predicted Label')\n",
    "axes[1, 0].set_ylabel('True Label')\n",
    "axes[1, 0].set_title('Confusion Matrix')\n",
    "\n",
    "# Model parameters comparison\n",
    "models_params = [\n",
    "    model_improved.count_params(),\n",
    "    model_lightweight.count_params(),\n",
    "    model_deep.count_params()\n",
    "]\n",
    "model_names_short = ['Improved\\nLeNet', 'Lightweight\\nCNN', 'Deep\\nCNN']\n",
    "axes[1, 1].bar(model_names_short, models_params, color=['blue', 'green', 'red'], alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Models')\n",
    "axes[1, 1].set_ylabel('Parameters')\n",
    "axes[1, 1].set_title('Model Parameters Comparison')\n",
    "for i, v in enumerate(models_params):\n",
    "    axes[1, 1].text(i, v + max(models_params) * 0.01, f'{v:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ccb912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
